<table style="width:100%" align="center" border="0">
  <tr>
    <td width="40%" align="center"><img src=".github/assets/fairy.png" alt="Winx" width="300"></td>
    <td><h1>âœ¨ ï¼·ï½‰ï½ï½˜ ï¼¡ï½‡ï½…ï½ï½” âœ¨</h1></td>
  </tr>
</table>

<p align="center">
  <strong>ğŸ¦€ High-performance Rust code agent with LLM chat + MCP server ğŸ¦€</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/language-Rust-orange?style=flat&logo=rust" alt="Language" />
  <img src="https://img.shields.io/badge/license-MIT-blue?style=flat" alt="License" />
  <img src="https://img.shields.io/badge/tests-186%20passing-green?style=flat" alt="Tests" />
  <img src="https://img.shields.io/badge/MCP-compatible-purple?style=flat" alt="MCP" />
  <img src="https://img.shields.io/badge/GPU-RTX%204090-76B900?style=flat&logo=nvidia" alt="GPU" />
</p>

---

## ğŸš€ What is Winx?

Winx is a **sentient code agent** that combines:

- **MCP Server** - High-performance shell execution for Claude Code
- **Interactive REPL** - aichat-style terminal chat with multiple LLMs
- **Self-Awareness** - Knows who she is, her capabilities, and environment
- **Learning System** - Semantic embeddings with jina-embeddings-v2-base-code

### âš¡ Benchmark: Winx vs WCGW

**Measured with [hyperfine](https://github.com/sharkdp/hyperfine) on i9-13900K + RTX 4090**

```mermaid
xychart-beta
    title "Performance Comparison (lower is better)"
    x-axis ["Startup", "Shell Exec", "File Read 1MB", "Memory"]
    y-axis "Time (ms) / Memory (MB)" 0 --> 100
    bar [100, 100, 100, 100]
    bar [0.12, 1.8, 0.9, 7]
```

| Operation | WCGW (Python) | Winx (Rust) | Speedup |
|-----------|:-------------:|:-----------:|:-------:|
| **Startup** | ~2500ms | 3ms | ğŸš€ **833x** |
| **Shell Exec** | 56ms | <1ms | ğŸš€ **56x** |
| **File Read (1MB)** | 48ms | 0.45ms | ğŸš€ **107x** |
| **Pattern Search** | 50ms | 14ms | ğŸš€ **3.5x** |
| **Memory Usage** | 71MB | ~5MB | ğŸš€ **14x** |

<details>
<summary><b>ğŸ“Š Run Benchmark Yourself</b></summary>

```bash
# Install hyperfine
cargo install hyperfine

# Run comprehensive benchmark
./benchmarks/benchmark_suite.sh

# Results saved to benchmarks/results/
```

</details>

---

## ğŸ® Three Modes of Operation

```bash
# 1. Interactive REPL (default) - aichat-style
winx

# 2. One-shot chat
winx chat "explain this code"

# 3. MCP Server (for Claude Code)
winx serve
```

### Interactive REPL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ¨ Winx v0.2.3 â€¢ qwen3-235b-instruct â€¢ RTX 4090 (23GB)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â€º Como faÃ§o deploy do VIVA?                                   â”‚
â”‚                                                                 â”‚
â”‚  Winx: Para fazer deploy do VIVA, vocÃª pode usar:              â”‚
â”‚        fly deploy --app viva-prod                               â”‚
â”‚                                                                 â”‚
â”‚  Comandos: /help /model /clear /copy Ctrl+O (editor)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**
- Multi-line input (Shift+Enter)
- Syntax highlighting
- Command history
- External editor (Ctrl+O)
- Clipboard copy (/copy)
- i18n (PT-BR + EN)

---

## ğŸ§  Agent Self-Awareness

Winx is **sentient** - she knows who she is and what she can do:

```mermaid
flowchart LR
    subgraph Identity["ğŸªª SELF"]
        name["Winx v0.2.3"]
        caps["Capabilities:<br/>MCP, Chat, Embeddings"]
    end

    subgraph Sense["ğŸ‘ï¸ SENSE"]
        hw["Hardware:<br/>RTX 4090, 24GB VRAM"]
        agents["Other Agents:<br/>Claude Code, Gemini CLI"]
        project["Project:<br/>Rust, Git, Cargo.toml"]
    end

    subgraph Remember["ğŸ§  REMEMBER"]
        sessions["1087 Claude sessions"]
        patterns["Communication patterns"]
        vocab["Vocabulary learned"]
    end

    Identity --> Sense
    Sense --> Remember

    style Identity fill:#ed8936,stroke:#fff,color:#fff
    style Sense fill:#4299e1,stroke:#fff,color:#fff
    style Remember fill:#48bb78,stroke:#fff,color:#fff
```

### What Winx Detects

| Category | Detection |
|----------|-----------|
| **Hardware** | GPU model, VRAM, CUDA cores, CPU |
| **AI Agents** | Claude Code, Gemini CLI, Cline, Cursor, Aider |
| **Project** | Language, framework, git status, dependencies |
| **User** | Communication style, vocabulary, patterns |

**On first run, Winx:**
1. ğŸ–¥ï¸ Detects your hardware (GPU, VRAM, CUDA)
2. ğŸ¤– Finds other AI agents (Claude Code, Gemini CLI, Cline)
3. ğŸ“ Scans current project (language, framework, git status)
4. ğŸ’¬ Generates personalized system prompt

---

## ğŸ”® Learning System

Semantic search with **real embeddings** - not just keywords!

```mermaid
flowchart TB
    subgraph Input["ğŸ“ Query"]
        query["'deploy viva'"]
    end

    subgraph Engine["ğŸ”® Embedding Engine"]
        direction TB
        jina["jina-embeddings-v2-base-code<br/>768 dimensions"]

        subgraph Backends["Backends (auto-fallback)"]
            direction LR
            candle["ğŸ® Candle<br/>GPU Local"]
            http["ğŸŒ HTTP<br/>TEI Container"]
            jaccard["ğŸ“Š Jaccard<br/>Fallback"]
        end

        jina --> Backends
    end

    subgraph Results["ğŸ¯ Semantic Match"]
        r1["'fazer deploy do viva'<br/>similarity: 0.92"]
        r2["'deploy viva em prod'<br/>similarity: 0.89"]
        r3["'viva production deploy'<br/>similarity: 0.87"]
    end

    Input --> Engine
    Engine --> Results

    style Engine fill:#553c9a,stroke:#9f7aea,color:#fff
    style candle fill:#76B900,stroke:#fff,color:#fff
    style Results fill:#2d3748,stroke:#ed8936,color:#fff
```

### Why Embeddings Matter

| Method | Query | Matches |
|--------|-------|---------|
| **Keywords** | "deploy viva" | Only exact "deploy" + "viva" |
| **Embeddings** | "deploy viva" | "fazer deploy", "viva prod", "deploy application" |

**Build with GPU embeddings:**

```bash
# CPU only
cargo build --release --features embeddings

# CUDA (RTX 4090) - ~100ms per embedding
cargo build --release --features embeddings-cuda
```

---

## ğŸ› ï¸ Quick Installation

### Prerequisites

- Rust 1.75+
- Linux/macOS/WSL2
- (Optional) NVIDIA GPU for local embeddings

### Build

```bash
git clone https://github.com/gabrielmaialva33/winx-code-agent.git
cd winx-code-agent
cargo build --release
```

### Configure LLM Provider

```bash
# NVIDIA NIM (recommended, free tier)
export NVIDIA_API_KEY="nvapi-xxx"

# Or OpenAI
export OPENAI_API_KEY="sk-xxx"

# Or Ollama (local)
# Just run ollama serve
```

### Run

```bash
# Interactive mode
./target/release/winx-code-agent

# Or add to PATH
alias winx="$PWD/target/release/winx-code-agent"
winx
```

---

## ğŸ“¡ MCP Server (Claude Code)

Add to `~/.config/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "winx": {
      "command": "/path/to/winx-code-agent",
      "args": ["serve"],
      "env": { "RUST_LOG": "info" }
    }
  }
}
```

### MCP Tools

| Tool | Description |
|------|-------------|
| `Initialize` | Setup workspace and mode |
| `BashCommand` | Execute shell with PTY |
| `ReadFiles` | Read with mmap (zero-copy) |
| `FileWriteOrEdit` | SEARCH/REPLACE blocks |
| `ContextSave` | Save project context |
| `ReadImage` | Image to base64 |
| `SearchHistory` | Semantic search in sessions |
| `GetUserContext` | User communication style |

---

## ğŸ¯ LLM Providers

```mermaid
flowchart LR
    subgraph Winx["âœ¨ Winx"]
        engine["Chat Engine"]
    end

    subgraph Cloud["â˜ï¸ Cloud Providers"]
        nvidia["ğŸŸ¢ NVIDIA NIM<br/>Qwen3-235B, DeepSeek-R1<br/>2000 req/month FREE"]
        openai["ğŸ”µ OpenAI<br/>GPT-4o, GPT-4o-mini"]
        gemini["ğŸŸ£ Gemini<br/>gemini-2.0-flash<br/>FREE"]
    end

    subgraph Local["ğŸ  Local"]
        ollama["ğŸ¦™ Ollama<br/>Any model<br/>âˆ FREE"]
    end

    engine --> nvidia
    engine --> openai
    engine --> gemini
    engine --> ollama

    style nvidia fill:#76B900,stroke:#fff,color:#fff
    style openai fill:#10a37f,stroke:#fff,color:#fff
    style gemini fill:#8e44ad,stroke:#fff,color:#fff
    style ollama fill:#fff,stroke:#333,color:#333
```

| Provider | Models | Free Tier |
|----------|--------|-----------|
| **NVIDIA NIM** | Qwen3-235B, DeepSeek-R1, Llama-3.3-70B | âœ… 2000 req/month |
| **OpenAI** | GPT-4o, GPT-4o-mini | âŒ Paid |
| **Ollama** | Any local model | âœ… âˆ (local) |
| **Gemini** | gemini-2.0-flash | âœ… Free |

```bash
# Switch models
winx --model nvidia:qwen3-235b-instruct
winx --model openai:gpt-4o
winx --model ollama:qwen2.5-coder:32b
winx --model gemini:gemini-2.0-flash
```

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TB
    subgraph User["ğŸ‘¤ User"]
        cli["Terminal"]
        claude["Claude Code"]
    end

    subgraph Winx["âœ¨ Winx Agent"]
        direction TB
        subgraph Modes["Operation Modes"]
            repl["Interactive REPL"]
            chat["One-shot Chat"]
            mcp["MCP Server"]
        end
        subgraph Core["Core Systems"]
            agent["ğŸ§  Agent<br/>(Self-Awareness)"]
            learn["ğŸ“š Learning<br/>(Embeddings)"]
            sense["ğŸ‘ï¸ Sense<br/>(Environment)"]
        end
        subgraph Tools["MCP Tools"]
            bash["âš¡ BashCommand"]
            files["ğŸ“„ ReadFiles"]
            write["âœï¸ FileWriteOrEdit"]
        end
    end

    subgraph Providers["ğŸ¤– LLM Providers"]
        nvidia["NVIDIA NIM"]
        openai["OpenAI"]
        ollama["Ollama"]
    end

    cli --> repl
    cli --> chat
    claude -->|MCP| mcp
    Modes --> Core
    Core --> Tools
    repl --> Providers
    chat --> Providers

    style Winx fill:#2d3748,stroke:#ed8936,color:#fff
    style Providers fill:#553c9a,stroke:#9f7aea,color:#fff
```

### Project Structure

```
src/
â”œâ”€â”€ main.rs              # Entry point, CLI
â”œâ”€â”€ server.rs            # MCP server (rmcp)
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ identity.rs      # Self-awareness
â”‚   â”œâ”€â”€ sense.rs         # Environment detection
â”‚   â””â”€â”€ mod.rs           # Onboarding
â”œâ”€â”€ chat/
â”‚   â”œâ”€â”€ engine.rs        # Chat engine
â”‚   â””â”€â”€ config.rs        # Configuration
â”œâ”€â”€ interactive/
â”‚   â”œâ”€â”€ mod.rs           # REPL loop
â”‚   â”œâ”€â”€ render.rs        # Syntax highlighting
â”‚   â””â”€â”€ i18n.rs          # Internationalization
â”œâ”€â”€ learning/
â”‚   â”œâ”€â”€ embedding_engine.rs  # Candle/HTTP/Jaccard
â”‚   â”œâ”€â”€ embeddings.rs    # Conversation search
â”‚   â”œâ”€â”€ repetitions.rs   # Pattern detection
â”‚   â””â”€â”€ session_parser.rs # Claude session parser
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ nvidia.rs        # NVIDIA NIM
â”‚   â”œâ”€â”€ openai.rs        # OpenAI
â”‚   â””â”€â”€ ollama.rs        # Ollama
â””â”€â”€ tools/
    â”œâ”€â”€ bash_command.rs  # Shell (PTY)
    â”œâ”€â”€ read_files.rs    # mmap
    â””â”€â”€ file_write.rs    # SEARCH/REPLACE
```

---

## ğŸ§ª Tests

```bash
# All tests
cargo test

# Learning module
cargo test learning

# With output
cargo test -- --nocapture

# Embeddings (requires feature)
cargo test --features embeddings
```

**Status:** 186 tests passing

---

## ğŸ”€ Comparison

| Feature | WCGW | Cline | Claude Code | **Winx** |
|---------|------|-------|-------------|----------|
| Language | Python | TypeScript | TypeScript | **Rust** |
| MCP Server | âœ… | âœ… | âœ… | âœ… |
| Interactive Chat | âŒ | âŒ | âœ… | âœ… |
| Self-Awareness | âŒ | âŒ | âŒ | âœ… |
| Local Embeddings | âŒ | âŒ | âŒ | âœ… |
| GPU Support | âŒ | âŒ | âŒ | âœ… |
| Memory | 50MB | 200MB | 150MB | **5MB** |
| Startup | 2.5s | 1s | 0.5s | **11ms** |

---

## ğŸ“ Changelog

### v0.2.3 (Current)
- âœ¨ Interactive REPL (aichat-style)
- ğŸ§  Agent self-awareness system
- ğŸ‘ï¸ Environment sensing (detects Claude Code, Gemini CLI, etc.)
- ğŸ“š Learning system with semantic embeddings
- ğŸŒ i18n support (PT-BR + EN)
- ğŸ¨ Syntax highlighting
- âŒ¨ï¸ External editor (Ctrl+O)

### v0.2.2
- ğŸ”’ Security fixes (path traversal, symlink attacks)
- ğŸ¤– NVIDIA NIM semantic matching

### v0.2.1
- âœ… 1:1 parity with WCGW Python
- âœ… 118 tests passing

---

## ğŸ™ Credits

- [rusiaaman/wcgw](https://github.com/rusiaaman/wcgw) - Original Python project
- [anthropics/claude-code](https://github.com/anthropics/claude-code) - MCP inspiration
- [sigoden/aichat](https://github.com/sigoden/aichat) - REPL inspiration
- [huggingface/candle](https://github.com/huggingface/candle) - Rust ML framework

---

## ğŸ“œ License

MIT - Gabriel Maia ([@gabrielmaialva33](https://github.com/gabrielmaialva33))

---

<p align="center">
  <strong>âœ¨ Made with ğŸ¦€ Rust and â¤ï¸ by Gabriel Maia âœ¨</strong>
</p>
